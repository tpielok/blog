--- 

title: "Stein Variational Gradient Descent"
date: 2022-04-18 
usemathjax: true

---

![svgd](https://raw.githubusercontent.com/tpielok/blog/main/_images/svgd.svg)

{% if page.usemathjax %}
<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
{% endif %}

I created some [introductory slides](https://tpielok.github.io/presentations/svgd.html) about the [Stein Variational Gradient Descent](https://arxiv.org/abs/1608.04471) paper written by Q. Liu et al. and wanted to share some of the main insights:

Often, in probabilistic machine learning we are interested in finding the posterior density of a Bayesian experiment such that

$$p(\boldsymbol{\theta}\vert\boldsymbol{y}, \boldsymbol{X}) = \frac{p(\boldsymbol{y}|\boldsymbol{X}, \boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\boldsymbol{y}, \boldsymbol{X})}$$

with smooth likelihood $$p(\boldsymbol{y}\vert\boldsymbol{X}, \boldsymbol{\theta})$$, smooth prior $$p(\boldsymbol{\theta})$$ and marginal likelihood $$p(\boldsymbol{y}, \boldsymbol{X})$$ which is in general intractable. This is, e.g., the case for most Bayesian neural networks. 
